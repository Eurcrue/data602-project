{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_szn_df = pd.read_csv('./data/MRegularSeasonCompactResults.csv')\n",
    "reg_szn_df = reg_szn_df[reg_szn_df['Season'] == 2023]\n",
    "reg_szn_df['score_diff'] = reg_szn_df['WScore'] - reg_szn_df['LScore']\n",
    "reg_szn_df['days_till_tourney'] = 132 - reg_szn_df['DayNum']\n",
    "reg_szn_df = reg_szn_df[['WTeamID', 'LTeamID', 'score_diff', 'days_till_tourney']]\n",
    "reg_szn_df.columns = ['team1', 'team2', 'score_diff', 'days_till_tourney']\n",
    "\n",
    "data = reg_szn_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reynald/anaconda3/envs/sprvs/lib/python3.12/site-packages/pymc/sampling/jax.py:470: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  pmap_numpyro = MCMC(\n",
      "sample: 100%|██████████| 2000/2000 [01:45<00:00, 18.92it/s, 15 steps of size 2.58e-01. acc. prob=0.82]\n",
      "sample: 100%|██████████| 2000/2000 [01:41<00:00, 19.67it/s, 15 steps of size 2.63e-01. acc. prob=0.82]\n",
      "sample: 100%|██████████| 2000/2000 [01:35<00:00, 21.03it/s, 15 steps of size 2.86e-01. acc. prob=0.78]\n",
      "sample: 100%|██████████| 2000/2000 [01:42<00:00, 19.46it/s, 15 steps of size 2.72e-01. acc. prob=0.80]\n"
     ]
    }
   ],
   "source": [
    "teams = list(set(data['team1']).union(data['team2']))\n",
    "team_to_idx = {team: idx for idx, team in enumerate(teams)}\n",
    "data['team1_idx'] = data['team1'].map(team_to_idx)\n",
    "data['team2_idx'] = data['team2'].map(team_to_idx)\n",
    "\n",
    "# Convert team names to indices\n",
    "teams = list(set(data['team1']).union(data['team2']))\n",
    "team_to_idx = {team: idx for idx, team in enumerate(teams)}\n",
    "data['team1_idx'] = data['team1'].map(team_to_idx)\n",
    "data['team2_idx'] = data['team2'].map(team_to_idx)\n",
    "\n",
    "# Hyperparameters for recency weighting\n",
    "tau_decay = 0.01  # Controls how fast weights decay with time\n",
    "\n",
    "# Calculate weights for each game based on recency\n",
    "data['weight'] = np.exp(-tau_decay * data['days_till_tourney'])\n",
    "\n",
    "# Model\n",
    "with pm.Model() as model:\n",
    "    # Priors for team ratings (mean and variance)\n",
    "    team_means = pm.Normal(\"team_means\", mu=0, sigma=10, shape=len(teams))\n",
    "    team_vars = pm.HalfNormal(\"team_vars\", sigma=5, shape=len(teams))\n",
    "\n",
    "    # Calculate game-specific ratings differences\n",
    "    rating_diff = team_means[data['team1_idx']] - team_means[data['team2_idx']]\n",
    "\n",
    "    # Point differences follow a normal distribution\n",
    "    game_variances = team_vars[data['team1_idx']] + team_vars[data['team2_idx']]\n",
    "    game_dist = pm.Normal.dist(mu=rating_diff, sigma=pm.math.sqrt(game_variances))\n",
    "\n",
    "    # Weighted log-likelihood\n",
    "    weighted_logp = pm.logp(game_dist, data['score_diff']) * data['weight']\n",
    "\n",
    "    # Add the weighted likelihood to the model\n",
    "    pm.Potential(\"weighted_likelihood\", weighted_logp.sum())\n",
    "\n",
    "    # Sampling\n",
    "    trace = pm.sample(1000, \n",
    "                      chains = 4,\n",
    "                      cores = 1,\n",
    "                      return_inferencedata=True, \n",
    "                      progressbar=True,\n",
    "                      compute_convergence_checks=False,  # Speeds up sampling for large models\n",
    "                      nuts_sampler=\"numpyro\",\n",
    "                      nuts = {'target_accept': 0.7}\n",
    "                      )\n",
    "\n",
    "# Posterior mean and variance for each team\n",
    "team_ratings = {\n",
    "    team: {\n",
    "        \"mean\": trace.posterior[\"team_means\"].mean(dim=[\"chain\", \"draw\"]).values[team_to_idx[team]],\n",
    "        \"variance\": trace.posterior[\"team_vars\"].mean(dim=[\"chain\", \"draw\"]).values[team_to_idx[team]],\n",
    "    }\n",
    "    for team in teams\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.DataFrame(team_ratings).T\n",
    "ratings_df.index.name = 'TeamID'\n",
    "ratings_df = ratings_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_df = pd.read_csv('data/MNCAATourneyCompactResults.csv')\n",
    "tourney_df = tourney_df[tourney_df['Season'] == 2023][['WTeamID', 'LTeamID', 'WScore', 'LScore']]\n",
    "tourney_df['score_diff'] = tourney_df['WScore'] - tourney_df['LScore']\n",
    "tourney_df = tourney_df[['WTeamID', 'LTeamID', 'score_diff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(tourney_df, ratings_df, left_on = 'WTeamID', right_on = 'TeamID')\\\n",
    "                .drop(columns = ['WTeamID', 'TeamID'])\\\n",
    "                .rename(columns = {'mean': 'w_mean', 'variance': 'w_var'})\\\n",
    "                .merge(ratings_df, left_on = 'LTeamID', right_on = 'TeamID')\\\n",
    "                .drop(columns = ['LTeamID', 'TeamID'])\\\n",
    "                .rename(columns = {'mean': 'l_mean', 'variance': 'l_var'})\n",
    "merged_df['pred_score_diff'] = merged_df['w_mean'] - merged_df['l_mean']\n",
    "merged_df['pred_score_var'] = merged_df['w_var'] + merged_df['l_var']\n",
    "merged_df = merged_df[['score_diff', 'pred_score_diff', 'pred_score_var']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8882396297572314"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm as N\n",
    "\n",
    "-np.mean(np.log(N.sf(0,loc = merged_df['pred_score_diff'], scale = np.sqrt(merged_df['pred_score_var']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6119402985074627"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(N.sf(0,loc = merged_df['pred_score_diff'], scale = np.sqrt(merged_df['pred_score_var'])) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sprvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
